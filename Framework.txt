GIT SETUP/BASIC FILE INFRASTRUCTURE SETUP

1. Set up the git hub : create a new repository mlproject
	a) New environment: in system create a folder mlproject and open vs code there
		Then, create an environment using the code :
		conda create -p venv python == 3.8 -y
		
        Activate the enviornment just created: 
		> conda activate venv/
		Create a new file in vscode: readmme.md
 		Add this file in my git repository mlproject : 
		> git init
		> git add readme.md
		> git commit -m "First commit"
		> git status
		> cls [optional]
		> git branch -M main
		> git remote add origin https://github.com/learnermp/mlproject.git
		> git remote -v
		> git config --global user.name
		> git config --global user.email
		> git push -u origin main

		Create .gitignore on gihub followed by running the below mentioned code on terminal:
		> git pull

		b)  requirements.txt
		c) setup.py

--------------------------------------------------------------------

	from setuptools import find_packages, setup

	setup(
    name = 'mlproject',
    version = 0.0.1,
    author = 'Mrityunjay',
    author_email = learnermp@gmail.com,
    packages = find_packages(),
    install_requires = ['pandas', 'numpy', 'seaborn']
	)
    
---------------------------------------------------------------------

from setuptools import find_packages, setup
from typing import List

HYPHEN_E_DOT = "-e ."
def get_requirements(file_path:str)->List[str]:
    ''' 
    This function will return a list of rquirements

    '''
    requirements = []
    with open(file_path) as file_object:
        requirements = file_object.readlines()
        requirements = [req.replace('\n', '') for req in requirements]

        if HYPHEN_E_DOT in requirements:
            requirements.remove(HYPHEN_E_DOT)
    return requirements

setup(
name='mlproject',
version='0.0.1',
author='Mrityunjay',
author_email='learnermp@gmail.com',
packages=find_packages(),
install_requires=get_requirements('requirements.txt')
)
----------------------------------------------------------------------------
	d) create a folder src and within this folder create a file : __init__.py
		This helps building entire package and help in ulpoading to pypi, importing    
      etc. Whatever we are implementing in this project will come under this source called 'src'.

    > pip install -r requirements.txt

	 Creation of this folder 'mlproject.egg-info' indicated that packages have been installed.

    > git add .	
	> git commit -m " Second commit"
	> git push -u origin main


	e) create a folder uder 'src' called 'components' and within 'components' create '__init__.py', components will als be used as package and can be moved, imported, transported.	
	   create a file under 'src' called 'data_ingestion.py' for reading data from a database or another source. This file will have all the codes that will be used for reading the data.
       create a file under the folder 'src' named 'data_transformation.py' for transforming data.
	   create a file under the folder 'src' named 'model_trainer.py' for transforming data.
    "Components are basically modules that we will use step wise during our project"

   f) Create a folder under 'src' as 'pipeline'. Within this 'pipeline' we will create '__init__.py', 
    'train_pipeline.py' and 'predict_pipeline.py
	g) Create three files under 'src'
 folders - 1. 'utils.py' 2. 'logger.py' 3. 'exception.py'
    'utils.py' will have all other functionalities such as mongoclient(), model saving, cloud uploading code etc.

    h) search 'custom execption handling' in python

----------------------------------------------------------------------------
import sys   
'''
Any exception that is getting controlled sys library has that information
'''
def error_message_details(error, error_detail:sys):
    _,_,exc_tb = error_detail.exc_info() 
    # exc_tb variable has all important details such as kind, in which file, line number etc about error
    file_name = exc_tb.tb_frame.f_code.co_filename
    error_message = "Error occured in the python script name [{0}] line number [{}] error message [{3}]".format(file_name, exec_tb.tb_lineno, str(error))
    return error_message

class CustomException(Exception):
    def __init__(self, error_message, error_detail:sys):
        super.__init__(error_message)
        self.error_message = error_message_details(error_message, error_detail = error_detail)

    def __str__(self):
        return self.error_message
----------------------------------------------------------------------------
	 i) search 'logger' documentation in python

-------------------------
logger will enable to give textual output of all execution details like time, library installation, 
exceptions generated etc.
'''
import logging
import os
from datetime import datetime

LOG_FILE = f"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log"
logs_path = os.path.join(os.getcwd(), 'logs', LOG_FILE)
os.makedirs(logs_path, exist_ok=True)

LOG_FILE_PATH = os.path.join(logs_path, LOG_FILE)
logging.basicConfig(
    filename = LOG_FILE_PATH,
    format =  "[%(asctime)s] %(lineno)d %(name)s - %(levelname)s - %(message)s",
    level = logging.INFO)

if __name__=='__main__':
    logging.info("Logging has started...")
-----------------------------------------------
	> in terminal run: src/logger.py

----------------------------------------------------
import sys 
from src.logger import logging
'''
Any exception that is getting controlled sys library has that information
'''
def error_message_details(error, error_detail:sys):
    _,_,exc_tb = error_detail.exc_info() 
    # exc_tb variable has all important details such as kind, in which file, line number etc about error
    file_name = exc_tb.tb_frame.f_code.co_filename
    error_message = "Error occured in the python script name [{0}] line number [{1}] error message [{2}]".format(file_name, exc_tb.tb_lineno, str(error))
    return error_message

class CustomException(Exception):
    def __init__(self, error_message, error_detail:sys):
        super().__init__(error_message)
        self.error_message = error_message_details(error_message, error_detail = error_detail)

    def __str__(self):
        return self.error_message
    
# if __name__=='__main__':
#     try:
#         a = 1/0
#     except Exception as e:
#         logging.info("Divide by zero error..")
#         raise CustomException(e, sys)
-------------------------------------------------------------------------------------------------
2. EDA on jupyter

If during import pandas, numpy etc. the message pops up 'ipkernel install' appears in the VS Code then in terminal run :
> conda install jupyter

--------------------------------------------------------------------------------------------------
3. Model building on Jupyter

--------------------------------------------------------------------------------------------------
4. Data Ingestion coding

src> components > data_ingestion.py
 # Big data team fetch data from various resources and store them in a data base, mongoDB, hadoop etc.

import os
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from dataclasses import dataclass # used to create class variables

from src.exception import CustomException
from src.logger import logging
from src.components.data_transformation import DataTransformation, DataTransformationConfig
from src.components.model_trainer import ModelTraininerConfig, ModelTrainer


@dataclass
class DataIngestionConfig:
    train_data_path: str = os.path.join('artifacts','train.csv')        # train data will saved in this path
    test_data_path: str = os.path.join('artifacts','test.csv')         # these three inputs wwe are giving to data ingestion component
    raw_data_path: str = os.path.join('artifacts','data.csv')   # artifacts are basically folders. 

class DataIngestion:
    def __init__(self):
        self.ingestion_config = DataIngestionConfig()

    def initiate_data_ingestion(self):   # if data is stored in database, the code for reading that data will be written here
        logging.info("Entered data ingestion method or components")
        try:
            df = pd.read_csv(r'C:\Users\mriyu\OneDrive\Desktop\Data_Science-LAPTOP-GMML798J\1ML Projects\mlproject\notebook\data\stud.csv')
            # here to df the data source could be mySQL, mongoDB, any APIs or something else
            logging.info("Read the dataset asmdataframe")

            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)

            df.to_csv(self.ingestion_config.raw_data_path, index =False, header =True)

            logging.info("train test split initiated")

            train_set, test_set = train_test_split(df, test_size= 0.2, random_state=42)

            train_set.to_csv(self.ingestion_config.train_data_path, index =False, header =True)

            test_set.to_csv(self.ingestion_config.test_data_path, index =False, header =True)

            logging.info("Ingestion of the data completed")

            return(
                self.ingestion_config.train_data_path, self.ingestion_config.test_data_path
            )
                    
        except Exception as e:
            raise CustomException
    
if __name__ == "__main__":
    obj = DataIngestion()
    train_data, test_data = obj.initiate_data_ingestion()

    data_transformation = DataTransformation()
    train_arr, test_arr,_ = data_transformation.initiate_data_transformation(train_data, test_data)

    modeltrainer = ModelTrainer()
-----------------------------------------------------------

5. data_transformation.py

import sys
from dataclasses import dataclass

import numpy as np 
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder,StandardScaler

from src.exception import CustomException
from src.logger import logging
import os

from src.utils import save_object

@dataclass
class DataTransformationConfig:
    preprocessor_obj_file_path=os.path.join('artifacts',"proprocessor.pkl")

class DataTransformation:
    def __init__(self):
        self.data_transformation_config=DataTransformationConfig()

    def get_data_transformer_object(self):
        '''
        This function si responsible for data trnasformation
        
        '''
        try:
            numerical_columns = ["writing_score", "reading_score"]
            categorical_columns = [
                "gender",
                "race_ethnicity",
                "parental_level_of_education",
                "lunch",
                "test_preparation_course",
            ]

            num_pipeline= Pipeline(
                steps=[
                ("imputer",SimpleImputer(strategy="median")),
                ("scaler",StandardScaler())

                ]
            )

            cat_pipeline=Pipeline(

                steps=[
                ("imputer",SimpleImputer(strategy="most_frequent")),
                ("one_hot_encoder",OneHotEncoder()),
                ("scaler",StandardScaler(with_mean=False))
                ]

            )

            logging.info(f"Categorical columns: {categorical_columns}")
            logging.info(f"Numerical columns: {numerical_columns}")

            preprocessor=ColumnTransformer(
                [
                ("num_pipeline",num_pipeline,numerical_columns),
                ("cat_pipelines",cat_pipeline,categorical_columns)

                ]


            )

            return preprocessor
        
        except Exception as e:
            raise CustomException(e,sys)
        
    def initiate_data_transformation(self,train_path,test_path):

        try:
            train_df=pd.read_csv(train_path)
            test_df=pd.read_csv(test_path)

            logging.info("Read train and test data completed")

            logging.info("Obtaining preprocessing object")

            preprocessing_obj=self.get_data_transformer_object()

            target_column_name="math_score"
            numerical_columns = ["writing_score", "reading_score"]

            input_feature_train_df=train_df.drop(columns=[target_column_name],axis=1)
            target_feature_train_df=train_df[target_column_name]

            input_feature_test_df=test_df.drop(columns=[target_column_name],axis=1)
            target_feature_test_df=test_df[target_column_name]

            logging.info(
                f"Applying preprocessing object on training dataframe and testing dataframe."
            )

            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)
            input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df)

            train_arr = np.c_[
                input_feature_train_arr, np.array(target_feature_train_df)
            ]
            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]

            logging.info(f"Saved preprocessing object.")

            save_object(

                file_path=self.data_transformation_config.preprocessor_obj_file_path,
                obj=preprocessing_obj

            )

            return (
                train_arr,
                test_arr,
                self.data_transformation_config.preprocessor_obj_file_path,
            )
        except Exception as e:
            raise CustomException(e,sys)     
       
		
-----------------------------------
6. model_trainer.py

import os
import sys
from dataclasses import dataclass

from src.exception import CustomException
from src.logger import logging
from src.utils import save_object, evaluate_model

from sklearn.ensemble import (AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor)
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score

from xgboost import XGBRegressor
from catboost import CatBoostRegressor

@dataclass
class ModelTraininerConfig:
    trained_model_file_path = os.path.join("artifacts", "model.pkl")

class ModelTrainer:
    def __init__(self):
        self.model_trainer_config = ModelTraininerConfig()


    def initiate_model_trainer(self, train_array, test_array):
        try:
            logging.info("splitting trainig and test input data")
            X_train, y_train,X_test, y_test = (train_array[:,:-1], train_array[:,-1], test_array[:,:-1], test_array[:,-1])
       

            models ={"Random Forest": RandomForestRegressor(),
                    "Decision Tree": DecisionTreeRegressor(),
                    "Gradient Boosting": GradientBoostingRegressor(),
                    "Linear Regression": LinearRegression(),
                    "XGBRegressor": XGBRegressor(),
                    "CatBoosting Regressor": CatBoostRegressor(verbose=False),
                    "AdaBoost Regressor": AdaBoostRegressor()}
            
            model_report:dict = evaluate_model(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, 
                                            models =models)
            
            # to get best model score from dict
            best_model_score = max(sorted(model_report.values()))

            # to get best model name from dict

            best_model_name = list(model_report.keys())[list(model_report.values()).index(best_model_score)]

            best_model = models[best_model_name]

            if best_model_score < 0.6:
                raise CustomException("No best model found")
            
            logging.info(f'Best found model on both training and test set')

            save_object(
                file_path = self.model_trainer_config.trained_model_file_path,
                obj = best_model
            )

            predicted =  best_model.predict(X_test)

            r2_squared = r2_score(y_test, predicted)
            return r2_squared
    
        except Exception as e:
            raise CustomException(e, sys)


  -----------------------------------------------

7. ....

Deployment: .ebextension provides an instance that will be deployed on cloud.



    
